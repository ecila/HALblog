---
layout: page
title: "SCHEDULE"
header: no
image:
  title: "grays_lato_img.gif"

permalink: "/schedule&speakers/"
---


### Day 1 Presentations & discussion

+ 10:30 : Coffee
+ 11:00 : Welcome
+ 11:30 : Presentations: Archival & Technical
+ 13:00 : lunch
+ 14:00 : Presentations: Critical & Philosophical
+ 15:30 : Coffee Break & Feedback Cello Installation
+ 16:15 : Break out group discussions - agenda brainstorming
+ 18.15 : Fin

Dinner in Brighton


### Day 2 - Kaggle challenge


+ 09:30 : Coffee
+ 10:00 : Round Table - initial agenda scoping
+ 12:30 : Kaggle briefing and team formation
+ 13:00 : Lunch Break
+ 14:00 : Kaggle challenge brain storming
+ 15:30 : Coffee & tea break
+ 16:00 : Kaggle team presentations & vote
+ 17:00 : Roundtable discussion and summary of the day
+ 18.00 : Fin

----

----
## <a name="speakers"></a> SPEAKERS
----

----
![image of Ron Chrisley]({{site.urlimg}}p_chrisley.jpg)
### **Ron Chrisley**
*What would it be for a robot to sing?*  
<p></p>
Drawing on my experiences from investigating this question in the context of a Nao robot, I outline a number of relevant dimensions such as dis/embodiment, playback vs. synthesis, skill, and the potential for lyricism.  I focus on the role that a singing robot’s own listening capability can/should play in its performance and/or acquisition of singing skills.

My academic connection with computers and sound started as an undergraduate at Stanford.  Each of my close friends from my freshman dorm has gone on to make a mark in computers and sound, whether it be inventing the Shazam algorithm, founding a key Hollywood audio editing firm (Audio Mechanics), patenting a sound spatialisation chip widely used in Soundblaster cards in the 80s and 90s, or leading one of the speech recognition teams behind Google Voice.  Like them, I studied computer music under John Chowning and digital signal processing under Julius Smith.  I also studied music theory and experimental forms of music composition.  After Stanford, I worked on neural networks for speech recognition with Teuvo Kohonen in Helsinki, and at ATR in Japan.  My current related interests include expectation-based models of sensory experience, including audition, and the aesthetic and engineering challenges of answering the question:  What would it be for a robot to sing?

<p></p>
----
![image of Tristan Clutterbuck]({{site.urlimg}}p_clutterbuck.jpg)
### **Tristan Clutterbuck**
*Listening is not listening, content is not content*
<p></p>
In spite of continuous efforts to describe the relationship between listening, content, and meaning in music - a stable and widely applicable account remains elusive. Each lacking the integrity or stability to reach much further than a description of the context of the breath which utters them. My interest lies in how interpretations of machine agency / listening interact with, and feed-back into, how we describe and construct meaningful human practices. How metaphors of intentional human behaviour are constructed through the complex network of system outputs, user experiences, and system author descriptions (Jichen Zhu). Leaning on actor-network theory and embodied cognition, this talk questions the usefulness of these metaphors, and examines the heuristic relationships between content, context, and meaning that constitute current machine-listening practices. Envisioning a speculative, explicitly ‘multiscale’ and ecological approach to machine listening - one which questions the ethics of outsourcing the practices of categorisation and group formation to the computational.

Tristan Clutterbuck is a PhD candidate at the Sonic Arts Research Centre whose partly practice-based research draws from interests in; complex musical instruments, machine agency, the history of music technology, improvisation, sociology of music, and actor-network theory.

<p></p>
<p></p>
----
![image of Fiona Courage]({{site.urlimg}}p_courage.jpg)
### **Fiona Courage**
*Describing and accessing sound recordings*
<p></p>
Archivists have long been recipients and keepers of records, from scrolls through to hard copy memos and scribbled letters from the papers of individuals and institutions. Whilst traditional methods of cataloguing have served to describe these ‘traditional’ types of record, a century of technological development has presented archivists with new formats to preserve and describe, including sound recordings. This presentation will concentrate on the challenges and opportunities that are faced by archivists in describing and accessing sound recordings, looking to the opportunities that new technologies and methods may provide to open up sound archives.

Fiona Courage is the University of Sussex Special Collections Manager and Curator of the Mass Observation Archive, based at The Keep. Having trained as a librarian, she is responsible for the care and accessibility of the University’s collection of manuscripts, archives and rare books, and works closely with academic and professional colleagues in looking at ways to open up accessibility to heritage collections.


<p></p>
<p></p>
----
![image of Steven Dorrestijn]({{site.urlimg}}p_Dorrestijn.jpg)
### **Steven Dorrestijn**
*Being heard and remembered: Technical mediation and what it means to be human*
<p></p>
The technical mediation approach in the philosophy of technology considers that technology is more than just an external factor that can support and hinder human life. Technology is constitutive for human self-understanding and for what it means to be human. What does this mean for an ethics of technology? Must we learn to take care of our hybrid selves? For this occasion, I will especially refer to the impact of recording and transporting speech.


Steven Dorrestijn is a senior researcher in the Ethics and Technology research group at Saxion University of Applied Sciences, the Netherlands. Dorrestijn’s research and publications focus on the philosophy and ethics of technology. He has a broad, cross-disciplinary interest in examples and conceptualizations of how technology mediates human existence. To make such insights about the impacts of technology on humans useful for the assessment and design of technologies he has develops the “Product Impact Tool”. For ethical reflection on living technically mediated lives Dorrestijn has specialized in the work of Michel Foucault and technology.
In 2012 Dorrestijn completed his PhD thesis, The design of our own lives: Technical mediation and subjectivation after Michel Foucault, at the University of Twente, the Netherlands. Previously he studied Philosophy in Paris and Philosophy and Mechanical Engineering in Twente.


<p></p>
<p></p>
----

![image of Beatrice Fazi]({{site.urlimg}}p_fazi_beatrice.jpg)
### **Beatrice Fazi**
*Listening with machines that are already ‘listening’? From Augmentation to Automation*
<p></p>
*What does it mean to listen with or through machines that are already ‘listening’?* I engage with this question by examining the anthropomorphism that might be implicit in the notion of ‘listening algorithms’.  This will help me to argue for the difference between functions and processes of ‘augmentation’ on the one hand, and of ‘automation’ on the other. I will argue that whilst augmentation implies the extension and exteriorisation of predefined forms or modes of behaviour, contemporary developments in computational automation ask us instead to consider the possibility to move beyond a simulative paradigm or phenomenological analogies.


Beatrice Fazi is Research Fellow in Digital Humanities & Computational Culture at the Sussex Humanities Lab (University of Sussex, United Kingdom). Her primary areas of expertise are the philosophy of computation, the philosophy of technology and the new emerging field of media philosophy. Beatrice’s current work investigates the limits and potentialities of formal reasoning in relation to computation, and aims to offer a re-conceptualisation of contingency within formal axiomatic systems vis-à-vis technoscientific notions of incompleteness and incomputability. This research is part of a monograph that she is currently writing on how indeterminacy shapes the ontological foundation of computational aesthetics.
<p></p>
<p></p>

----

![Image of Tim Hitchcock]({{site.urlimg}}p_hitchcock15.jpg)
### **Tim Hitchcock**
*Listening to the Dead*
<p></p>
The traces of the past historians normally rely upon are made up of rotting print and fragile manuscript.  But a proportion of this material represents speech uttered in known environments – courtrooms, churches and parliamentary chambers.  And as these traces of the dead have been digitised and turned in to a new kind of 'object of study' (searchable, mash-upable, and macroscopable) we are increasingly challenged to analyse them in light of all the other forms of data that intersect with mere textual recordings   This presentation briefly suggest that the addition of a quantifiable understanding of sound (reflecting historical spaces and environments) to a ‘big data’ approach to textual representations of historical speech, allows us to understand the meaning and import of that speech (and inherited text) in a fundamentally new way.  It allows to listen to the dead, in hopes of hearing the timbre and rhythms of their words.

Tim Hitchcock is Professor of Digital History at the University of Sussex, and co-director of the Sussex Humanities Lab.  A historian of eighteenth and nineteenth century London, Hitchcock has published widely on poverty, sexuality and crime.  With Robert Shoemaker, he has also been responsible for half a dozen major web resources making searchable and re-usable some 35 billion words of historical text and several hundred thousand images.

<p></p>
<p></p>

----
![Image of Parag Mital]({{site.urlimg}}p_parag.png)
### **Parag Mital**
*Auditory Perception and Attention and computational arts*
<p></p>
I will present a cursory overview of fMRI and EEG literature relating to auditory perception and attention mechanisms, behavioral science of auditory attention, and detail some computational investigations for understanding audio within a computational arts practice.

Parag K. MITAL (US) is an artist and interdisciplinary researcher obsessed with the nature of information, representation, and attention. Using film, eye-tracking, EEG, and fMRI recordings, he has worked on computational models of audiovisual perception from the perspective of both robots and humans, often revealing the disjunct between the two, through generative film experiences, augmented reality hallucinations, and expressive control of large audiovisual corpora. Through this process, he balances his scientific and arts practice, with both reflecting on each other: the science driving the theories, and the artwork re-defining the questions asked within the research. His work has been exhibited internationally including the Prix Ars Electronica, ACM Multimedia, Victoria & Albert Museum, London’s Science Museum, Oberhausen Short Film Festival, and the British Film Institute, and featured in FastCompany, BBC, NYTimes, CreativeApplications.Net, and CreateDigitalMotion.

<p></p>
<p></p>
----
![image of Shintaro Miyazaki]({{site.urlimg}}p_shintaro_IAMAS_2015.jpg)
### **Shintaro Miyazaki**
*Listening to Algorhythmics*
<p></p>

The aim is to provide a probably diametrically opposed approach to “machine listening“ via a media archaeological inquiry into algorhythmic listening in the era between 1940–1965, where mainframe machine operators and scientists were listening to their computing machinery. I will provide some further implementations for the digital humanities context.

Shintaro Miyazaki is a Berlin-born Swiss-Japanese media and design scholar and experimental media designer. He has been a Senior Researcher at the Critical Media Lab of the Institute of Experimental Design and Media Cultures, Academy of Art and Design part of the University of Applied Sciences and Arts Northwestern Switzerland in Basel since 2014 and since 2016 principle investigator there. Shintaro obtained a PhD in media theory at Humboldt-Universität in Berlin (2012). His current interests include cybernetics, design theory and research, non-visual modes of knowledge.
<p></p>
<p></p>

----
![SJN]({{site.urlimg}}p_sjn.jpg)
### **Sally-Jane Norman**
*Tuning (with/ to/ by) Exosomatic Organs*
<p></p>

The Old English term *hlysnan* designates listening, hearing, and paying attention, and musicking history abounds in technical artefacts designed to extend our listening abilities. These exosomatic organs (Robert Innis) augment human expressivity, soliciting their creative users and audiences by employing and deploying algorithmic functions – sets of rules or processes. Drawing on music history, I will argue that cultural tuning is key to humanising emerging algorithmic listening practices.

Sally-Jane Norman is Professor of Performance Technologies and Co-Director of Sussex Humanities Lab where she leads the  ‘Digital Technologies, Digital Performance’ strand. She joined Sussex after serving as founding Director of Culture Lab at Newcastle University, Research Director at the Institut International de la Marionnette (Charleville-Mézières), and Artistic Co-Director of STEIM (co-organiser of the first Touch Festival with Michel Waisvisz and Joel Ryan). She is a dual citizen of Aotearoa/ New Zealand and France, trandisciplinary performance scholar (Doctorat d’état, Paris III) and sometime practitioner.  From July, Sally Jane will become Director of Te Koki - New Zealand School of Music, at Victoria University in Wellington, Aotearoa.

<p></p>
<p></p>
----

![DS]({{site.urlimg}}p_danStowell.jpeg)
### **Dan Stowell**
*Computers Listening to Birds*
<p></p>

The sounds that birds make are a fascinating challenge for algorithmic listening. Often there is evidence of much complexity, yet how little we understand about the content and purpose of each individual sound. (Whereas with human sounds, we fool ourselves into thinking that we do understand the content and purpose.) I will outline the ways in which we have developed machine listening methods adapted to specific aspects of bird vocalisation - its fine details and its temporal structure - and how the challenge relates to the wider field of machine listening.

Dan Stowell is a researcher in machine listening - which means using computation to understand sound signals. He co-leads the Machine Listening Lab at Queen Mary University of London, based in the Centre for Digital Music. Dan has worked on voice, music and environmental soundscapes, and is currently leading a five-year EPSRC fellowship project researching the automatic analysis of bird sounds. His first degree was from Cambridge University, and his PhD from Queen Mary University of London.
