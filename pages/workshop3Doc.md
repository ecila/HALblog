---
layout: page
title:  WORKSHOP 3 DOCUMENTATION
header: no
image:
  title: "grays_lato_img.gif"

permalink: "/W3documentation/"
---

### Future Implications and Applications of Algorithmic Listening
Sept 14-15. University of Sussex, Brighton   


## Day 1
----
----

<iframe width="100%" height="546" src="https://www.youtube.com/embed/zpwmOxHUyfA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
### **Alice Eldridge & Paul Stapleton**
*Introduction*


----
<iframe width="100%" height="546" src="https://www.youtube.com/embed/0cWFI1yZe0U" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

### **Hanns Holger Rutz**
*Algorithms that Matter*
<p></p>


Algorithms that Matter (Almat) is a three-year FWF-funded artistic research project (AR 403) run by Hanns Holger Rutz and David Pirrò at the IEM Graz. It asks how algorithmic processes emerge and structure the praxis of experimental computer music. What we are interested in is to rethink algorithms as agents co-determining the boundary between an artistic machine or “apparatus” and the object produced through this machine. Unravelling the seemingly stable notion of algorithm, we look at the way an experimental culture in which the work with algorithms is embedded shapes our understanding of it, retroacts and changes the very praxis of composition and performance. Using as dispositifs two distinct software systems we have created, _SoundProcesses_ and _rattle_, we implement a series of connected experiments, addressing research questions such as: What are the “units” of algorithms, in what way can they be de- and recomposed, what is the nature of their affordances and material traces, how can they be preserved and inform the methodology of artistic research? Special focus is put on the reconfiguration of elements, such as relaying a system to another artist/composer, and we work with a number of invited guest artists to explore the different algorithmic strategies.   
[https://almat.iem.at/](https://almat.iem.at/)


<p></p>
---
<iframe width="100%" height="546" src="https://www.youtube.com/embed/rypLw7piFG4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>


### **Palle Dahlstedt**
*Systemic Improvisation*
<p></p>

Systemic Improvisation is a research project emerging from a series of musical works by Palle Dahlstedt and Per Anders Nilsson, exploring how computer-mediated interactions between improvisers can lead to new music. Here, an improvisation system is a configuration of human and virtual agents with communication going in both directions. Systemic improvisation is the act of a number of musicians playing in such a system. The virtual agents take input from human players, and produce visual or aural cues, which the human players react to. Together they form a complex interconnected system, with characteristic emergent behavior. Through design of (and playing on) a series of such systems, we study how musical output and musicians' experiences depend on system configuration, cue type, time scale, and a number of other design parameters.

<p></p>
----
<iframe width="100%" height="546" src="https://www.youtube.com/embed/tfrL3RovmTY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

### **Q&A**
<p></p>


----
<iframe width="100%" height="546" src="https://www.youtube.com/embed/r3V2kqPcgGY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

### **Oi, Algorithm! Chew On This! - Assaying the Noise Between Human and Algorithm**
*John Bowers & Owen Green*
<p></p>
We take the position that what it is to be human and what it is to be algorithmic can be creatively considered as variable affairs, complexly intertwined, co-indexical and so constitute a domain of investigation which is labile, uncertain and profoundly noisy. We propose to map some of this territory through a series of design provocations and a large portfolio of small collaborative makes, which are assembled in performance or installation, and critically reflected upon in the light of the concerns of the Network.

We will give a short performance-lecture, seguing into discussion that details the impetus and goals for our collaboration. 

<p></p>

----
----


### **Adaptive & Reflexive Musical Listening Algorithms**


<p></p>

<iframe width="100%" height="546" src="https://www.youtube.com/embed/UEuuScCvSBw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
#### **A sense of being ‘listened to’**   
*Nicholas Ward & Tom Davis*   

<p></p>
How can a sense of being ‘listened to’ effect human-algorithm listening relationships? Drawing on ideas informed by the use of auditory cues in conversation, facial expression in interview techniques, and the sense of being stared at, Nick and Tom will present some things from their practice before probing the audience for insights.
<p></p>

----

<iframe width="100%" height="546" src="https://www.youtube.com/embed/VhgDcFVv_rY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

#### **Self-listening for music generation**   
*Ron Chrisley*
<p></p>
Although it may seem obvious that in order to create interesting music one must be capable of listening to music as music, the ability to listen is often omitted in the design of musical generative systems.  And for those few systems that can listen, the emphasis is almost exclusively on listening to others, e.g., for the purposes of interactive improvisation.  The project aims to explore the role that a system’s listening to, and evaluating, that system's own musical performance (as its own musical performance) can play in musical generative systems.  What kinds of aesthetic and creative possibilities are afforded by such a design? How does the role of self-listening change at different timescales? Can self-listening generative systems shed light on neglected aspects of human performance?  A three-component architecture for answering questions such as these will be presented.
<p></p>


----

<iframe width="100%" height="546" src="https://www.youtube.com/embed/qQh4W7mI9gM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

#### **Q&A**  

---
---
## Day 2
---
---



<iframe width="100%" height="546" src="https://www.youtube.com/embed/rl4gmPXHX4w" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

### **Listening with dynamical and chaotic systems**   
*Marije Baalman & Chris Kiefer*   
<p></p>
What makes dynamical systems different from other approaches to algorithmic listening, and what are the best uses of these algorithms?  We'll explore dynamical systems hands-on, by doing some processing of live sensor data, and discuss their efficacy and applications.
<p></p>
----
----

### **Audio Feature Surfing**

<p></p>
<iframe width="100%" height="546" src="https://www.youtube.com/embed/IwxtqajMM3o" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

#### **Getting to Know Big Audio Data** -- *Alice Eldridge*
<p></p>
When machine listening methods are used to address empirical questions -- or even to look for general qualitative trends --  we necessarily need to work with audio data sets which exceed listenable compass. We can't listen to it all, and this can hamper interpretation of subsequent models. I will present some existing research in the visualisation of long form audio recordings and invite participants to brain-storm /hack around with ideas for perceptualisation of audio features which might afford a rapid, deep listening for large audio databases.
<p></p>

----


<p> </p>

<iframe width="100%" height="546" src="https://www.youtube.com/embed/bN2SVuXmhy0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
#### **Patrice Guyot**
*Feature Challenge - response*

----
<iframe width="100%" height="546" src="https://www.youtube.com/embed/6XVJRFt2hKQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

#### **Introduction to CataRT**
*Diemo Schwarz*
The concatenative real-time sound synthesis system [CataRT]([http://imtr.ircam.fr/imtr/CataRT](http://imtr.ircam.fr/imtr/CataRT)) plays grains from a large corpus of segmented and descriptor-analysed sounds according to proximity to a target position in the descriptor space. This can be seen as a content-based extension to granular synthesis providing direct access to specific sound characteristics.

CataRT is implemented in MaxMSP and takes full advantage of the generalised data structures and arbitrary-rate sound processing facilities of the FTM and Gabor libraries. Segmentation and sound descriptors are loaded from text or SDIF files, or analysed on-the-fly.

CataRT allows to explore the corpus interactively or via a target sequencer, to resynthesise an audio file or live input with the source sounds, or to experiment with expressive speech synthesis and gestural control.

CataRT is explained in more detail [in this article](http://recherche.ircam.fr/equipes/analyse-synthese/schwarz/publications/dafx2006/catart-dafx2006-long.pdf) and is an interactive implementation of the new concept of [Corpus-Based Concatenative Synthesis](http://imtr.ircam.fr/imtr/Corpus_Based_Synthesis).

----

<p> </p>

<iframe width="100%" height="546" src="https://www.youtube.com/embed/VxY4uaonx1A" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
#### **Dan Stowell**
*Feature Challenge - response*

----

<iframe width="100%" height="546" src="https://www.youtube.com/embed/R1xaA03v4UA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
#### **Parag Mital**
*Feature Challenge - response*

----

<iframe width="100%" height="546" src="https://www.youtube.com/embed/lvE0QV1ObFQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
#### **Simon Waters, Parag Mital, David Kant (Chairs)**
*Closing Panel*

----


<p></p>
<p></p>
